{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING GREAT NO ID GENERATION 02-18-2025\n",
    "\n",
    "import difflib\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "\n",
    "SUFFIXES = {\n",
    "    'MD', 'PhD', 'DDS', 'DVM', 'Esq', 'Jr', 'Sr', 'II', 'III', 'IV', 'V', 'VI',\n",
    "    'CPA', 'CFA', 'PE', 'MBE', 'OBE', 'CBE', 'KBE', 'DBE', 'RN', 'NP', 'RPh',\n",
    "    'Ret', 'Emeritus', 'USN', 'USA', 'USAF', 'MS', 'MA', 'MBA', 'JD', 'LLM',\n",
    "    'ThD', 'DMin', 'MTh', 'FAIA', 'FAAN', 'FACS', 'DPT', 'DC', 'DSc', 'MEng',\n",
    "    'MPH', 'MFA', 'MLIS', 'EdD', 'PsyD', 'DPhil', 'DPA', 'DNP', 'DO', 'DM',\n",
    "    'FRCS', 'FRCP', 'FRCOG', 'FRCPsych', 'FRSC', 'DCL', 'DLitt', 'DEng',\n",
    "    'FCA', 'FCMA', 'CGMA', 'CMgr', 'ChFC', 'CLU', 'CFP', 'AIA', 'FIA', 'ASA',\n",
    "    'ACAS', 'FCAS', 'FIA', 'MActSc', 'MSc', 'BSc', 'BA', 'BS', 'BEng', 'LLB'\n",
    "}\n",
    "\n",
    "PREFIXES = {\n",
    "    'Dr', 'Prof', 'Mr', 'Mrs', 'Ms', 'Miss', 'Mx', 'Master', 'Sir', 'Dame',\n",
    "    'Lady', 'Lord', 'Hon', 'Rev', 'Fr', 'Rabbi', 'Imam', 'Sheikh', 'Capt',\n",
    "    'Col', 'Maj', 'Lt', 'Cpl', 'Sgt', 'Gen', 'Adm', 'President', 'Senator',\n",
    "    'Governor', 'Ambassador', 'Judge', 'Justice', 'Attorney', 'Esq', 'Professor',\n",
    "    'The Honorable', 'Sr', 'Elder', 'Brother', 'Sister', 'Pastor', 'Bishop',\n",
    "    'Archbishop', 'Cardinal', 'Pope', 'Deacon', 'Venerable', 'Canon', 'Chaplain',\n",
    "    'Father', 'Mother', 'Abbess', 'Prior', 'Abbot', 'Metropolitan', 'Ayatollah',\n",
    "    'Mullah', 'Reverend Dr', 'Minister', 'Chancellor', 'Principal', 'Provost',\n",
    "    'Dean', 'Regent', 'Chairman', 'Chairwoman', 'Commander', 'Commodore',\n",
    "    'Brigadier', 'Marshal', 'Field Marshal', 'Rear Admiral', 'Vice Admiral',\n",
    "    'Air Chief Marshal', 'Air Marshal', 'Air Vice Marshal', 'General of the Army',\n",
    "    'Fleet Admiral', 'Supreme Commander', 'Grand Master', 'Warden', 'Sovereign'\n",
    "}\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"Robust cleaning with encoding repair and middle initial preservation\"\"\"\n",
    "    # Fix multi-layer encoding errors\n",
    "    name = fix_text(name, normalization='NFC')\n",
    "    \n",
    "    # Normalize Unicode and preserve middle initials\n",
    "    name = unicodedata.normalize('NFKD', name)\n",
    "    name = ''.join(c for c in name if not unicodedata.combining(c))\n",
    "    \n",
    "    # Transliterate special characters\n",
    "    name = unidecode(name)\n",
    "    \n",
    "    # Enhanced middle initial detection\n",
    "    name = re.sub(r'\\b([A-Z])[.]?\\s+', r'\\1. ', name)  # Standardize initials\n",
    "    \n",
    "    # Remove prefixes/suffixes\n",
    "    for prefix in sorted(PREFIXES, key=len, reverse=True):\n",
    "        name = re.sub(fr'(?i)^\\s*{re.escape(prefix)}\\b[.,]?\\s*', '', name)\n",
    "    \n",
    "    for suffix in sorted(SUFFIXES, key=len, reverse=True):\n",
    "        name = re.sub(fr'(?i)\\s*\\b{re.escape(suffix)}[.,]?\\s*$', '', name)\n",
    "    \n",
    "    # Final cleanup\n",
    "    name = re.sub(r'[^\\w\\s.]', ' ', name)  # Keep periods for initials\n",
    "    name = re.sub(r'\\s+', ' ', name).strip().title()\n",
    "    \n",
    "    return name\n",
    "\n",
    "def group_names(names):\n",
    "    def parse_name(name):\n",
    "        cleaned = clean_name(name)\n",
    "        parts = cleaned.split()\n",
    "        return {\n",
    "            'original': name,\n",
    "            'first': parts[0] if parts else '',\n",
    "            'middles': parts[1:-1] if len(parts) > 2 else [],\n",
    "            'last': parts[-1] if parts else ''\n",
    "        }\n",
    "\n",
    "    def is_initial(part):\n",
    "        cleaned = part.replace('.', '').strip()\n",
    "        return len(cleaned) == 1 and cleaned.isalpha()\n",
    "\n",
    "    parsed = [parse_name(name) for name in names]\n",
    "    last_name_groups = {}\n",
    "    \n",
    "    for p in parsed:\n",
    "        last = p['last']\n",
    "        last_name_groups.setdefault(last, []).append(p)\n",
    "\n",
    "    grouped_results = {}\n",
    "\n",
    "\n",
    "    for last, group in last_name_groups.items():\n",
    "        # Process first names\n",
    "        full_firsts = set()\n",
    "        initial_firsts = {}\n",
    "        for p in group:\n",
    "            first = p['first']\n",
    "            if is_initial(first):\n",
    "                initial = first.replace('.', '').upper()\n",
    "                initial_firsts.setdefault(initial, []).append(p)\n",
    "            else:\n",
    "                full_firsts.add(first)\n",
    "\n",
    "        # Create first name mapping\n",
    "        first_initial_map = {}\n",
    "        for initial in initial_firsts:\n",
    "            candidates = [name for name in full_firsts if name.upper().startswith(initial)]\n",
    "            if candidates:\n",
    "                first_initial_map[initial] = sorted(candidates)[0]\n",
    "\n",
    "        # Process middle names\n",
    "        middle_initial_map = {}\n",
    "        for p in group:\n",
    "            for middle in p['middles']:\n",
    "                if not is_initial(middle) and middle:\n",
    "                    initial = middle[0].upper()\n",
    "                    middle_initial_map.setdefault(initial, middle)\n",
    "\n",
    "        # Standardize and group names\n",
    "        for p in group:\n",
    "            # Standardize first name\n",
    "            original_first = p['first']\n",
    "            if is_initial(original_first):\n",
    "                initial = original_first.replace('.', '').upper()\n",
    "                std_first = first_initial_map.get(initial, original_first)\n",
    "            else:\n",
    "                std_first = original_first\n",
    "\n",
    "            # Standardize middle names\n",
    "            std_middles = []\n",
    "            for middle in p['middles']:\n",
    "                if is_initial(middle):\n",
    "                    initial = middle.replace('.', '').upper()\n",
    "                    std_middles.append(middle_initial_map.get(initial, middle))\n",
    "                else:\n",
    "                    std_middles.append(middle)\n",
    "\n",
    "            # Create standardized key\n",
    "            std_key = (\n",
    "                std_first.strip().title(),\n",
    "                ' '.join(std_middles).strip(),\n",
    "                last.strip().title()\n",
    "            )\n",
    "            \n",
    "            original = p['original']  # Use original name\n",
    "            if std_key not in grouped_results:\n",
    "                grouped_results[std_key] = []\n",
    "            grouped_results[std_key].append(original)\n",
    "            \n",
    "    merge_map = {}\n",
    "    for key in list(grouped_results.keys()):\n",
    "        first, middles, last = key\n",
    "        base_key = (first, last)\n",
    "        \n",
    "        # Extract initials from middles\n",
    "        current_initials = [m[0].upper() for m in middles.split() if m.strip()]\n",
    "        \n",
    "        if base_key not in merge_map:\n",
    "            merge_map[base_key] = {\n",
    "                'main_key': key,\n",
    "                'variations': grouped_results[key],\n",
    "                'initials': current_initials,\n",
    "                'middle_length': len(middles.split())\n",
    "            }\n",
    "        else:\n",
    "            existing = merge_map[base_key]\n",
    "            # Check if middles are compatible\n",
    "            existing_initials = existing['initials']\n",
    "            \n",
    "            # Merge if initials match or are subset\n",
    "            if (set(current_initials).issubset(existing_initials) or\n",
    "                set(existing_initials).issubset(current_initials)):\n",
    "                \n",
    "                existing['variations'].extend(grouped_results[key])\n",
    "                # Keep longest middle name format\n",
    "                if len(middles.split()) > existing['middle_length']:\n",
    "                    existing['main_key'] = key\n",
    "                    existing['initials'] = current_initials\n",
    "                    existing['middle_length'] = len(middles.split())\n",
    "                del grouped_results[key]\n",
    "\n",
    "    for entry in merge_map.values():\n",
    "        if entry['main_key'] not in grouped_results:\n",
    "            grouped_results[entry['main_key']] = entry['variations']\n",
    "\n",
    "    # Prepare final output groups\n",
    "    final_groups = []\n",
    "    for (first, middles, last), variations in grouped_results.items():\n",
    "        canonical_parts = [first]\n",
    "        if middles:\n",
    "            canonical_parts.extend(middles.split())\n",
    "        canonical_parts.append(last)\n",
    "        final_groups.append({\n",
    "            'canonical_name': ' '.join(canonical_parts),\n",
    "            'variations': sorted(list(set(variations)))\n",
    "        })\n",
    "\n",
    "    final_groups.sort(key=lambda x: x['canonical_name'])\n",
    "    return final_groups\n",
    "\n",
    "def cluster_protected_names(names, eps=0.15):  # Reduced eps for tighter clusters\n",
    "    # Clean names and get rule groups\n",
    "    cleaned_names = [clean_name(n) for n in names]\n",
    "    rule_groups = group_names(names)\n",
    "    \n",
    "    # Create name to canonical mapping\n",
    "    name_to_canonical = {}\n",
    "    for group in rule_groups:\n",
    "        for variation in group['variations']:\n",
    "            name_to_canonical[variation] = group['canonical_name']\n",
    "    \n",
    "    # Build distance matrix using canonical names\n",
    "    n = len(names)\n",
    "    distance_matrix = np.ones((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if name_to_canonical[names[i]] == name_to_canonical[names[j]]:\n",
    "                distance_matrix[i,j] = 0.0  # Force same cluster for same group\n",
    "            else:\n",
    "                # Compare cleaned canonical names\n",
    "                canon_i = clean_name(name_to_canonical.get(names[i], names[i]))\n",
    "                canon_j = clean_name(name_to_canonical.get(names[j], names[j]))\n",
    "                sim = difflib.SequenceMatcher(None, canon_i, canon_j).ratio()\n",
    "                distance_matrix[i,j] = 1 - sim\n",
    "            if i == j:\n",
    "                distance_matrix[i,j] = 0\n",
    "\n",
    "    # Cluster with DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=1, metric='precomputed')\n",
    "    clusters = dbscan.fit_predict(distance_matrix)\n",
    "    \n",
    "    # Organize clusters\n",
    "    cluster_dict = {}\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        cluster_dict.setdefault(cluster_id, []).append(names[idx])\n",
    "    \n",
    "    return cluster_dict, rule_groups\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    names = pd.read_excel(\"trial_output_1.xlsx\")['name'].tolist()\n",
    "    \n",
    "    clusters, rule_groups = cluster_protected_names(names, eps=0.175)\n",
    "    \n",
    "    print(\"Cleaned and Protected Clusters:\")\n",
    "    for cluster_id, members in clusters.items():\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        print(\" • \" + \"\\n • \".join(members))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
